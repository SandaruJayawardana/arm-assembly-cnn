asm("
push {r4,r5,r6,r7,r8,r9,r10,r12};
vpush {s16-s29};

mov r10,#0;//current layer
vmov s6,r10;//to compare with 0 for leaky activation
ldr r2,[r2];//leaky constant
vmov s7,r2;//leaky constant


// r0<-ArrayStart, r1<-depth, r2<-leaky constant, r3<-kernalStart, r4<- i, r5<-j, r6<-currentAddr,r8<- currentKernal,r9<-save location,r10<- current layer,r12 <- No allocate

//calculate save location________________________________________________________________
ldr r7,[r1],#+4;//r7=width of image*4
ldr r1,[r1]; //depth
mov r9,r1,LSL #7;
add r9,r9,r3;

//********************************  CONV SECTION *****************************************//

mov r6,r0;//image start


//padding________________________________________________________________________________
//it check whether the output has even or odd length
mov r4,#2;
sub r0,r7,#8;
LSR r0,r0,#3;
mov r2,#1;
and r2,r2,r0;
cmp r10,r2;
bne pad_ea; // jump to odd section
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
pad_a:;
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
cmp r0,r4;
add r4,r4,#2;
bne pad_a;

b pad_exi;

pad_ea:;// odd section
sub r0,r0,#1;
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
pad_b:;
str r10,[r9],#+4;// str 0
str r10,[r9],#+4;// str 0
cmp r0,r4;
add r4,r4,#2;
bne pad_b;

pad_exi:;
//END of pad___________________________________________________________________________________

push {r9};// To bipass the location to after conv section


//layer change_________________________________________________________________________________
layerr:;
mov r4,#16;//start from 2
mov r5,#16;//start from 2

//store kernal {s20-s28}
vldmia r3!,{s20-s28};//load kernal q5,q6,s28

//convolution for single layer_________________________________________________________________

conv:;
mov r8,r6;//store starting addr r6=current position

//load image 3 pix
vldmia r8,{s16-s18};
add r8,r8,r7;//point to 2 row of image
vldmia r8!,{s19};//store 4th pixel
vmul.f32 q3,q4,q5;//first 4 pixels
vldmia r8,{s16-s17};//last 2 pixels of 2nd row
sub r8,r8,#4;
add r8,r8,r7;//point to 3rd row of image
vldmia r8!,{s18-s19};//last 2 pixels of 2nd row
vfma.f32 q3,q4,q6;//first 4 pixels
vldr s16,[r8];
vmul.f32 s16,s16,s28;
vadd.f32 s0,s12,s13;
vadd.f32 s0,s0,s14;
vadd.f32 s0,s0,s15;
vadd.f32 s0,s0,s16;

//start second conv

//image start addr pointer
add r8,r6,#4;
//load image 3 pix
vldmia r8,{s16-s18};
add r8,r8,r7;//point to 2 row of image
vldmia r8!,{s19};//store 4th pixel
vmul.f32 q3,q4,q5;//first 4 pixels
vldmia r8,{s16-s17};//last 2 pixels of 2nd row
sub r8,r8,#4;
add r8,r8,r7;//point to 3rd row of image
vldmia r8!,{s18-s19};//last 2 pixels of 2nd row
vfma.f32 q3,q4,q6;//first 4 pixels
vldr s16,[r8];
vmul.f32 s16,s16,s28;
vadd.f32 s1,s12,s13;
vadd.f32 s1,s1,s14;
vadd.f32 s1,s1,s15;
vadd.f32 s1,s1,s16;

//start third conv

//image start addr pointer
add r8,r6,r7;
//load image 3 pix
vldmia r8,{s16-s18};
add r8,r8,r7;//point to 2 row of image
vldmia r8!,{s19};//store 4th pixel
vmul.f32 q3,q4,q5;//first 4 pixels
vldmia r8,{s16-s17};//last 2 pixels of 2nd row
sub r8,r8,#4;
add r8,r8,r7;//point to 3rd row of image
vldmia r8!,{s18-s19};//last 2 pixels of 2nd row
vfma.f32 q3,q4,q6;//first 4 pixels
vldr s16,[r8];
vmul.f32 s16,s16,s28;
vadd.f32 s2,s12,s13;
vadd.f32 s2,s2,s14;
vadd.f32 s2,s2,s15;
vadd.f32 s2,s2,s16;

//start forth conv

//image start addr pointer
add r8,r6,r7;
add r8,r8,#4;
vldmia r8,{s16-s18};
add r8,r8,r7;//point to 2 row of image
vldmia r8!,{s19};//store 4th pixel
vmul.f32 q3,q4,q5;//first 4 pixels
vldmia r8,{s16-s17};//last 2 pixels of 2nd row
sub r8,r8,#4;
add r8,r8,r7;//point to 3rd row of image
vldmia r8!,{s18-s19};//last 2 pixels of 2nd row
vfma.f32 q3,q4,q6;//first 4 pixels
vldr s16,[r8];
vmul.f32 s16,s16,s28;
vadd.f32 s3,s12,s13;
vadd.f32 s3,s3,s14;
vadd.f32 s3,s3,s15;
vadd.f32 s3,s3,s16;

//store four convs
vstr s0,[r9];
add r9,r9,#4;
vstr s1,[r9];
add r9,r9,#4;
vstr s2,[r9];
add r9,r9,#4;
vstr s3,[r9];
add r9,r9,#4;

//inner and outer loop________________________________________________________________________
cmp r4,r7;
beq j_d;
add r4,r4,#8; // increment save location
add r6,r6,#8;
b conv;
j_d:;
cmp r5,r7;
beq j_e;//exit
add r5,r5,#8;
add r6,r6,r7;
add r6,r6,#16;
mov r4,#16;
b conv;

//exit
j_e:;
add r10,r10,#1;
cmp r1,r10;// r1<- depth , r10 <- current depth
add r6,r6,r7,LSL #1;
add r6,r6,r7;
add r6,r6,#16;
bgt layerr;


//*********************************** END OF CONV ALL LAYERS ********************************//

//*********************************** ADDING 8 CONVED pixels TOGETHER IN ALL LAYERS**********//

//r0<-save location, r1<-depth, r2<-no of addr to shift a layer(X4), r3<-track begining of tunnel, r4<- current addr, 
//r5<-j, r6<-(current) sum of 4 convs groups ,r8<- output layer row position, r9<-width of out put layer, r10<- #0 ,r12 <- current layer no

pop {r3};//r3<- track start position(begining of conved pixels)
mov r0,r3;//save location
mov r10,#0;
sub r2,r7,#8; // width of convoled layer in X4
mov r9,r2,LSR#3;//no of 4 blocks per row in output layer
mul r2,r2,r9;// calculate addr amount for layer shift 
LSL r2,r2,#1;
mov r5,r2,LSR #4;// no 4 conv groups in layer
mov r6,#2;// no of 4 convs
mov r8,#0;// pading pointer

//even 4 groups per row
//mov r5,r5,LSR #1;

summ_b:;
mov r12,#2;// no of layers
mov r4,r3;//r4<- currrent position
vldmia r4,{s16-s23};

summ_a:;
add r4,r4,r2;
vldmia r4,{s24-s31};
vadd.f32 q4,q4,q6;
vadd.f32 q5,q5,q7;
cmp r1,r12; //r1=layers
add r12,r12,#1;
bne summ_a;

//leaky activation  (out = (x>0)? x:0.1x) //s6=0 //s7=0.1

vcmp.f32 s16,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_a;//branch greater than
vmul.f32 s16,s16,s7;

skip_a:;

vcmp.f32 s17,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_b;//branch greater than
vmul.f32 s17,s17,s7;

skip_b:;

vcmp.f32 s18,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_c;//branch greater than
vmul.f32 s18,s18,s7;

skip_c:;

vcmp.f32 s19,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_d;//branch greater than
vmul.f32 s19,s19,s7;

skip_d:;

vcmp.f32 s20,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_e;//branch greater than
vmul.f32 s20,s20,s7;

skip_e:;

vcmp.f32 s21,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_f;//branch greater than
vmul.f32 s21,s21,s7;

skip_f:;

vcmp.f32 s22,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_g;//branch greater than
vmul.f32 s22,s22,s7;

skip_g:;

vcmp.f32 s23,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_h;//branch greater than
vmul.f32 s23,s23,s7;

skip_h:;

//Max pooling _________________________________________________________________________________

vcmp.f32 s16,s17;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_a;
vcmp.f32 s18,s17;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_b;
vcmp.f32 s19,s17;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_c;
vstr s1,[r0];
b j_me;
j_a:;
vcmp.f32 s18,s16;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_b;
vcmp.f32 s19,s16;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_c;
vstr s16,[r0];
b j_me;

j_b:;
vcmp.f32 s19,s18;
vmrs APSR_nzcv,fpscr;//update cpsr reg

bgt j_c;
vstr s18,[r0];
b j_me;

j_c:;
vstr s19,[r0];
j_me:;
add r0,r0,#4;

//padding line at end__________________________________________________________________________
add r8,r8,#1;
cmp r9,r8; //r9<-no per line    r8<-current
bne skip_p;

str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
mov r8,#0;

skip_p:;

//end of padding _________________________________________________________________________________
//max pool
vcmp.f32 s20,s21;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_a_b;
vcmp.f32 s22,s21;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_b_b;
vcmp.f32 s23,s21;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_c_b;
vstr s21,[r0];
b j_me_b;
j_a_b:;
vcmp.f32 s22,s20;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_b_b;
vcmp.f32 s23,s20;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_c_b;
vstr s20,[r0];
b j_me_b;

j_b_b:;
vcmp.f32 s23,s22;
vmrs APSR_nzcv,fpscr;//update cpsr reg

bgt j_c_b;
vstr s22,[r0];
b j_me_b;

j_c_b:;
vstr s22,[r0];
j_me_b:;

add r0,r0,#4;
//EXIT Max pooling________________________________________________________________________

add r8,r8,#1;//line end padding___________________________________________________________
cmp r9,r8; //r9<-no per line    r8<-current
bne skip_p_b;

str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
mov r8,#0;

skip_p_b:;

add r6,r6,#2;// increment 4 group position
cmp r5,r6;//check 4 groups 
add r3,r3,#32;// set start position
bge summ_b;

mov r10,#1;
and r9,r9,r10;
cmp r10,r9;
bne skip_last;

//*************************************ODD NO OF OUTPUT LAYER WIDTH CORRECTION**************//

mov r4,r3;//r4<- currrent position
mov r12,#2;
vldmia r4,{s16-s19};

summ_a_b:;
add r4,r4,r2;
vldmia r4,{s24-s27};
vadd.f32 q4,q4,q6;
cmp r1,r12; //r1=layers
add r12,r12,#1;
bne summ_a_b;

//leaky activation  (out = (x>0)? x:0.1x) //s6=0 //s7=0.1

vcmp.f32 s16,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_i;//branch greater than
vmul.f32 s16,s16,s7;

skip_i:;

vcmp.f32 s17,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_j;//branch greater than
vmul.f32 s17,s17,s7;

skip_j:;

vcmp.f32 s18,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_k;//branch greater than
vmul.f32 s18,s18,s7;

skip_k:;

vcmp.f32 s19,s6;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bge skip_l;//branch greater than
vmul.f32 s19,s19,s7;

skip_l:;

vcmp.f32 s16,s17;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_a_c;
vcmp.f32 s18,s17;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_b_c;
vcmp.f32 s19,s17;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_c_c;
vstr s1,[r0];
b j_me_c;
j_a_c:;
vcmp.f32 s18,s16;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_b_c;
vcmp.f32 s19,s16;
vmrs APSR_nzcv,fpscr;//update cpsr reg
bgt j_c_c;
vstr s16,[r0];
b j_me_c;

j_b_c:;
vcmp.f32 s19,s18;
vmrs APSR_nzcv,fpscr;//update cpsr reg

bgt j_c_c;
vstr s18,[r0];
b j_me_c;

j_c_c:;
vstr s19,[r0];
j_me_c:;
add r0,r0,#4;
//*********************************END of odd no output layer width correction**************//

skip_last:;

//***************************************LAST PADDING**************************************//

mov r4,#2;
mov r10,#0;
sub r3,r7,#8;
LSR r3,r3,#3;
mov r2,#1;
and r2,r2,r3;
cmp r10,r2;
bne pad_ea_b;
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
pad_a_b:;
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
cmp r3,r4;
add r4,r4,#2;
bne pad_a_b;

b pad_exi_b;

pad_ea_b:;
sub r3,r3,#1;
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
pad_b_b:;
str r10,[r0],#+4;// str 0
str r10,[r0],#+4;// str 0
cmp r3,r4;
add r4,r4,#2;
bne pad_b_b;

pad_exi_b:;

pop {r4,r5,r6,r7,r8,r9,r10,r12};
vpop {s16-s29};